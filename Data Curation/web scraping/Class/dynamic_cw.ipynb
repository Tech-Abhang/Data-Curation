{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7526cf8d-356c-4481-998e-4a26b9246a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been exported Successfully\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the WebDriver (e.g., using Chrome)\n",
    "driver = webdriver.Chrome()  # Ensure you have the appropriate WebDriver installed\n",
    "\n",
    "url = \"https://www.forbes.com/powerful-brands/list/3/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Give the page some time to load the dynamic content\n",
    "time.sleep(5)  # Adjust the sleep time as needed\n",
    "\n",
    "# Extract the page source after the content is fully loaded\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Locate the table\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# Extract rows from the table\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "# Open a file to write data\n",
    "with open('forbes.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Loop through rows and extract cell data\n",
    "    for row in rows:  \n",
    "        # Find all the cells (use \"th\" instead of \"td\" if the row contains headers)\n",
    "        cells = row.find_all(\"td\")  \n",
    "    \n",
    "    # Extract and clean text from each cell\n",
    "    cell_data = []  # Create an empty list to store the row's data\n",
    "    for cell in cells:\n",
    "        text = cell.text.strip()  # Get text from the cell and remove extra spaces\n",
    "        cell_data.append(text)  # Add the cleaned text to the list\n",
    "    \n",
    "    # Write the processed row data to the CSV file\n",
    "    writer.writerow(cell_data)  \n",
    "\n",
    "print(\"Data has been exported Successfully\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d9b794c-0afb-4e48-822a-fdef7993f0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been exported successfully\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the WebDriver (e.g., using Chrome)\n",
    "driver = webdriver.Chrome()  # Ensure you have the appropriate WebDriver installed\n",
    "\n",
    "url = \"https://www.bloomberg.com/billionaires/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Give the page some time to load the dynamic content\n",
    "time.sleep(5)  # Adjust the sleep time as needed\n",
    "\n",
    "# Extract the page source after the content is fully loaded\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "rows = soup.find_all('div', class_='table-row')\n",
    "\n",
    "# Open a file to write data\n",
    "with open('bloomberg_new.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['Rank', 'Name', 'Total net worth', 'Last change', 'YTD change', 'Country / Region', 'Industry'])\n",
    "\n",
    "    # Loop through rows and extract cell data\n",
    "    for row in rows:\n",
    "            rank = row.find('div', class_='t-rank').text.strip()\n",
    "            name = row.find('div', class_='t-name').find('a').text.strip()\n",
    "            total_net_worth = row.find('div', class_='t-nw').text.strip()\n",
    "            last_change = row.find('div', class_='t-lcd').text.strip()\n",
    "            ytd_change = row.find('div', class_='t-ycd').text.strip()\n",
    "            country_region = row.find('div', class_='t-country').text.strip()\n",
    "            industry = row.find('div', class_='t-industry').text.strip()\n",
    "\n",
    "            # Write the row to the CSV file\n",
    "            writer.writerow([rank, name, total_net_worth, last_change, ytd_change, country_region, industry])\n",
    "\n",
    "print(\"Data has been exported successfully\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dedbb73-b55e-49fa-8dbc-8f24eca90253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been exported successfully\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the WebDriver (e.g., Chrome)\n",
    "driver = webdriver.Chrome()  # Ensure you have the appropriate WebDriver installed and in PATH\n",
    "\n",
    "# URL of the ICC Cricket Rankings page\n",
    "url = \"https://www.icc-cricket.com/rankings/batting/mens/odi\"\n",
    "driver.get(url)\n",
    "\n",
    "# Give the page some time to load the dynamic content\n",
    "time.sleep(5)  # Adjust the sleep time as necessary\n",
    "\n",
    "# Extract the page source after the content is fully loaded\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Find all rows in the table\n",
    "rows = soup.find_all('div', class_='si-table-row')\n",
    "\n",
    "# Open a file to write data\n",
    "with open('icc_cricket_rankings.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['Position', 'Player', 'Team', 'Rating', 'Best'])\n",
    "\n",
    "    # Loop through rows and extract cell data\n",
    "    for row in rows:\n",
    "        try:\n",
    "            position = row.find('div', class_='si-pos').text.strip()\n",
    "            player = row.find('div', class_='si-player').text.strip()\n",
    "            team = row.find('div', class_='si-team').text.strip()\n",
    "            rating = row.find('div', class_='si-rating').text.strip()\n",
    "            best = row.find('div', class_='si-best').text.strip()\n",
    "\n",
    "            # Write the row to the CSV file\n",
    "            writer.writerow([position, player, team, rating, best])\n",
    "        except AttributeError:\n",
    "            # Skip rows that do not contain valid data\n",
    "            continue\n",
    "\n",
    "print(\"Data has been exported successfully\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14b709-f5e3-40c1-8751-050ea005399b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
